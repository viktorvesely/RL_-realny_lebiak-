{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from agent import AgentTypes as BT\n",
    "\n",
    "def ranks_eval(ranks, save=False, path=\"\", title=\"Density over ranks\"):\n",
    "    ranks, counts = np.unique(ranks, return_counts=True)\n",
    "    counts = counts / np.sum(counts)\n",
    "    ranks = ranks + 1\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:green'\n",
    "    ax1.set_xlabel('Rank of action')\n",
    "    ax1.set_ylabel('Portion of agents')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax1.bar(ranks, counts, color=color)\n",
    "    ax1.title.set_text(title)\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def trace_eval(traces, labels, save=False, path=\"\", title=\"Accuracy over time\"):\n",
    "    ts = np.arange(1, traces[0].size + 1)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('t')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.title.set_text(title)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    for trace, label in zip(traces, labels):\n",
    "        ax1.plot(ts, trace, label=label, linewidth=0.8)\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def reward_eval(rewards, env, labels, save=False, path=\"\", title=\"Reward over time\"):\n",
    "    ts = np.arange(1, rewards[0].size + 1)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('t')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.title.set_text(title)\n",
    "    if env == \"G\":\n",
    "        ax1.set_ylim([0, 4])\n",
    "    elif env == \"B\":\n",
    "        ax1.set_ylim([0, 1])\n",
    "    \n",
    "    for trace, label in zip(rewards, labels):\n",
    "        ax1.plot(ts, trace, label=label, linewidth=0.8)\n",
    "    \n",
    "    ax1.legend()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "method = BT.AP\n",
    "envName = \"Gaussian\"\n",
    "k = 10\n",
    "N = 1000\n",
    "T = 500\n",
    "\n",
    "name = method.name.lower()\n",
    "env = envName[0]\n",
    "ranks, trace, reward = gym.gains(method, env, k=k, N=N, T=T)\n",
    "ranks_eval(\n",
    "        ranks, \n",
    "        save=True, \n",
    "        path=f\"./graphs2/{name}_{env}_ranks.png\",\n",
    "        title=f\"Portion of agents over action rank \\nfor {name} in {envName} environment\"  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "method = BT.SOFT_MAX\n",
    "envName = \"Bernoulli\"\n",
    "k = 10\n",
    "N = 1000\n",
    "T = 500\n",
    "\n",
    "name = method.name.lower()\n",
    "env = envName[0]\n",
    "ranks, trace, reward = gym.gains(method, env, k=k, N=N, T=T)\n",
    "ranks_eval(\n",
    "        ranks, \n",
    "        save=True, \n",
    "        path=f\"./graphs2/{name}_{env}_ranks.png\",\n",
    "        title=f\"Portion of agents over action rank \\nfor {name} in {envName} environment\"  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "method = BT.EPSILON_GREEDY\n",
    "envName = \"Bernoulli\"\n",
    "epsilon = 0.3\n",
    "k = 10\n",
    "N = 1000\n",
    "T = 500\n",
    "\n",
    "name = method.name.lower()\n",
    "env = envName[0]\n",
    "ranks, trace, reward = gym.gains(method, env, k=k, N=N, T=T, extra=epsilon)\n",
    "ranks_eval(\n",
    "        ranks, \n",
    "        save=False, \n",
    "        path=f\"./graphs2/{name}_{env}_ranks.png\",\n",
    "        title=f\"Portion of agents over action rank \\nfor {name} in {envName} environment\"  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "envs = [\"Gaussian\", \"Bernoulli\"]\n",
    "\n",
    "groups = [\n",
    "    [BT.GREEDY, BT.EPSILON_GREEDY, BT.OPTIMISTIC],\n",
    "    [BT.AP, BT.SOFT_MAX, BT.SOFT_MAX_Q_VALUES, BT.UCB]\n",
    "]\n",
    "\n",
    "k = 10\n",
    "N = 1000\n",
    "T = 500\n",
    "\n",
    "qScalesG = 5\n",
    "qScalesB = 2\n",
    "epsilon = 0.1\n",
    "\n",
    "def generate_graphs():\n",
    "    for method in BT:\n",
    "        name = str(method).split(\".\")[1].lower()\n",
    "        extra = None\n",
    "\n",
    "        if method == BT.EPSILON_GREEDY:\n",
    "            extra = epsilon\n",
    "\n",
    "        for envName in envs:\n",
    "            env = envName[0]\n",
    "\n",
    "            if method == BT.OPTIMISTIC:\n",
    "                extra = qScalesG if env == \"G\" else qScalesB\n",
    "\n",
    "            ranks, trace, reward = gym.gains(method, env, k=k, N=N, T=T, extra=extra)\n",
    "            trace_eval(\n",
    "                trace, \n",
    "                save=True, \n",
    "                path=f\"./graphs/{name}_{env}_trace.png\",\n",
    "                title=f\"Accuracy over time \\nfor {name} in {envName} environment\"  \n",
    "            )\n",
    "            ranks_eval(\n",
    "                ranks, \n",
    "                save=True, \n",
    "                path=f\"./graphs/{name}_{env}_ranks.png\",\n",
    "                title=f\"Portion of agents over action rank \\nfor {name} in {envName} environment\"  \n",
    "            )\n",
    "            reward_eval(\n",
    "                reward, \n",
    "                save=True, \n",
    "                path=f\"./graphs/{name}_{env}_reward.png\",\n",
    "                title=f\"Average reward of agents over time \\nfor {name} in {envName} environment\"  \n",
    "            )\n",
    "\n",
    "            \n",
    "def generate_graphs_groups(groups, version=\"\"):\n",
    "    for group in groups:\n",
    "        \n",
    "\n",
    "        for envName in envs:\n",
    "            env = envName[0]\n",
    "            names = \"\"\n",
    "            traces = []\n",
    "            rewards = []\n",
    "            \n",
    "            for method in group:\n",
    "                name = str(method).split(\".\")[1].lower()\n",
    "                names += f\"{name}-\"\n",
    "                extra = None\n",
    "\n",
    "                if method == BT.OPTIMISTIC:\n",
    "                    extra = qScalesG if env == \"G\" else qScalesB\n",
    "\n",
    "                if method == BT.EPSILON_GREEDY:\n",
    "                    extra = epsilon\n",
    "                \n",
    "                _, trace, reward = gym.gains(method, env, k=k, N=N, T=T, extra=extra)\n",
    "                traces.append(trace)\n",
    "                rewards.append(reward)\n",
    "\n",
    "            names = names[:-1]\n",
    "            labels = names.split(\"-\")\n",
    "\n",
    "            trace_eval(\n",
    "                traces,\n",
    "                labels,\n",
    "                save=True, \n",
    "                path=f\"./graphs{version}/{names}_{env}_trace.png\",\n",
    "                title=f\"Accuracy over time in {envName} environment\"  \n",
    "            )\n",
    "            reward_eval(\n",
    "                rewards,\n",
    "                env,\n",
    "                labels,\n",
    "                save=True, \n",
    "                path=f\"./graphs{version}/{names}_{env}_reward.png\",\n",
    "                title=f\"Average reward of agents in {envName} environment\"  \n",
    "            )\n",
    "\n",
    "generate_graphs_groups(groups, version=\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BanditTypes.GREEDY'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(BT.GREEDY)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
